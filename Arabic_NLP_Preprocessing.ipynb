{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Netways Part II – Machine Learning Data Pre-processing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ConbI5WB5LU4",
        "7FK4dE_33ZTy",
        "K8dDQGoBtNIG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfmbVM7AuPJy"
      },
      "source": [
        "# Part II – Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ConbI5WB5LU4"
      },
      "source": [
        "> ## English Pipeline (Works for english only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa1X4EVIzWhu",
        "outputId": "4b303336-8800-43f3-bcbb-fe589a2d6cfe"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.text import Tokenizer  \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "from keras.utils import plot_model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "import unicodedata\n",
        "import html\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4Ci-rcsxG8E"
      },
      "source": [
        "def remove_special_chars(text):\n",
        "    re1 = re.compile(r'  +')\n",
        "    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
        "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
        "    return x1\n",
        "\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "\n",
        "def remove_numbers(text):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "\n",
        "def remove_whitespaces(text):\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def remove_stopwords(words, stop_words):\n",
        "    \"\"\"\n",
        "    :param words:\n",
        "    :type words:\n",
        "    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "    or\n",
        "    from spacy.lang.en.stop_words import STOP_WORDS\n",
        "    :type stop_words:\n",
        "    :return:\n",
        "    :rtype:\n",
        "    \"\"\"\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in text\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in words]\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    \"\"\"Lemmatize words in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n",
        "\n",
        "def text2words(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = remove_special_chars(text)\n",
        "    text = remove_non_ascii(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = to_lowercase(text)\n",
        "    text = remove_numbers(text)\n",
        "    words = text2words(text)\n",
        "    words = remove_stopwords(words, stop_words)\n",
        "    words = lemmatize_words(words)\n",
        "    words = lemmatize_verbs(words)\n",
        "\n",
        "    return ''.join(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UOgpOYJrzKZk",
        "outputId": "24ffbaa6-10a1-4cb1-af59-89d7023cf4b4"
      },
      "source": [
        "normalize_text(\"the baby is crying cried\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'baby cry cry'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FK4dE_33ZTy"
      },
      "source": [
        "> ## Arabic Pipeline (Works for arabic only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZCxCfXs-UKh"
      },
      "source": [
        "%pip install camel-tools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K5y6D_8W43c"
      },
      "source": [
        "os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'\n",
        "\n",
        "!export | camel_data full"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs37LBAS7fwJ"
      },
      "source": [
        "# For removing punctuation\n",
        "import string\n",
        "# Some characters have different variants, for example the character 'ع' can also be represented \n",
        "# as 'ﻊ', 'ﻋ', 'ﻌ' depending on where it appears in a word. For all these forms, we would call 'ع' their canonical form.\n",
        "from camel_tools.utils.normalize import normalize_unicode\n",
        "\n",
        "\n",
        "# It is common for Arabic speakers to use shortcuts when typing Arabic text. \n",
        "# For example, the different variants of the letter alef ('ا', 'آ', 'أ', 'إ') may be typed as just 'ا'.\n",
        "# هل ذهبت إلى المكتبة؟\n",
        "# هل ذهبت الى المكتبة؟\n",
        "# هل ذهبت الي المكتبة؟\n",
        "# هل ذهبت الي المكتبه؟\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "\n",
        "# To split by whitespace and seperate punctuation, we currently provide the utility function\n",
        "# هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\n",
        "# ['هَلْ', 'ذَهَبْتَ', 'إِلَى', 'المَكْتَبَةِ', '؟']\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "\n",
        "\n",
        "# هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\n",
        "# هل ذهبت إلى المكتبة؟\n",
        "# To remove arabic diacritical marks\n",
        "# Either use this strategy or the strategy that involves adding the most likely\n",
        "# diacritical marks coming \n",
        "#### was not used\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "\n",
        "\n",
        "# Load a pretrained disambiguator to use with a tokenizer\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "\n",
        "# we can specify parameter to separate \"فذهب\" to \n",
        "# [\"ذهب\",\n",
        "#     \"+ف\"]\n",
        "# we can also specify parameters to add diacritical marks based on a maximum likelihod estimation algorithm\n",
        "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Removing punctuation from text\n",
        "# string.punctuation = !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "def remove_punctuation(text):\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  return text\n",
        "\n",
        "\n",
        "# Remove english (not best technique there could be better)\n",
        "def remove_eng(text):\n",
        "   return re.sub(r'[A-Za-z]', '', text)\n",
        "\n",
        "\n",
        "# Load a pretrained disambiguator to use with a tokenizer\n",
        "mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
        "\n",
        "# Add them all into one pipeline\n",
        "def normalize_text(text):\n",
        "  text = remove_punctuation(text)\n",
        "  text = remove_eng(text)\n",
        "  text = normalize_unicode(text)\n",
        "  text = normalize_alef_maksura_ar(text)\n",
        "  text = normalize_alef_ar(text)\n",
        "  text = normalize_teh_marbuta_ar(text)\n",
        "\n",
        "  text = simple_word_tokenize(text)\n",
        "\n",
        "  # We can output diacritized tokens by setting `diac=True`\n",
        "  tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True, diac=True)\n",
        "  return ' '.join(tokenizer.tokenize(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gXK6s5kuXLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f14136d-2a5d-4672-b88c-3b1c0a3a6858"
      },
      "source": [
        "arabic_text = 'لوريم ايبسوم هو نموذج افتراضي يوضع في التصاميم لتعرض على العميل ليتصور طريقه وضع النصوص بالتصاميم سواء كانت تصاميم مطبوعه ... بروشور او فلاير على سبيل المثال ... او نماذج www.justawebsite.com مواقع انترنت ... وعند موافقه العميل المبدئيه على التصميم يتم ازالة هذا النص من التصميم ويتم وضع النصوص النهائية المطلوبة للتصميم و يقول البعض ان وضع النصوص التجريبية بالتصميم قد تشغل justanemail@gmail.com المشاهد عن وضع الكثير من الملاحظات او الانتقادات للتصميم الاساسي . وخلافا للاعتقاد السائد فإن لوريم إيبسوم ليس نصا عشوائيا،ً بل إن له جذور في الأدب اللاتيني الكلاسيكي منذ العام 45 قبل الميلاد. من كتاب \" حول أقاصي الخير والشر \"'\n",
        "arabic_text.split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['لوريم',\n",
              " 'ايبسوم',\n",
              " 'هو',\n",
              " 'نموذج',\n",
              " 'افتراضي',\n",
              " 'يوضع',\n",
              " 'في',\n",
              " 'التصاميم',\n",
              " 'لتعرض',\n",
              " 'على',\n",
              " 'العميل',\n",
              " 'ليتصور',\n",
              " 'طريقه',\n",
              " 'وضع',\n",
              " 'النصوص',\n",
              " 'بالتصاميم',\n",
              " 'سواء',\n",
              " 'كانت',\n",
              " 'تصاميم',\n",
              " 'مطبوعه',\n",
              " '...',\n",
              " 'بروشور',\n",
              " 'او',\n",
              " 'فلاير',\n",
              " 'على',\n",
              " 'سبيل',\n",
              " 'المثال',\n",
              " '...',\n",
              " 'او',\n",
              " 'نماذج',\n",
              " 'www.justawebsite.com',\n",
              " 'مواقع',\n",
              " 'انترنت',\n",
              " '...',\n",
              " 'وعند',\n",
              " 'موافقه',\n",
              " 'العميل',\n",
              " 'المبدئيه',\n",
              " 'على',\n",
              " 'التصميم',\n",
              " 'يتم',\n",
              " 'ازالة',\n",
              " 'هذا',\n",
              " 'النص',\n",
              " 'من',\n",
              " 'التصميم',\n",
              " 'ويتم',\n",
              " 'وضع',\n",
              " 'النصوص',\n",
              " 'النهائية',\n",
              " 'المطلوبة',\n",
              " 'للتصميم',\n",
              " 'و',\n",
              " 'يقول',\n",
              " 'البعض',\n",
              " 'ان',\n",
              " 'وضع',\n",
              " 'النصوص',\n",
              " 'التجريبية',\n",
              " 'بالتصميم',\n",
              " 'قد',\n",
              " 'تشغل',\n",
              " 'justanemail@gmail.com',\n",
              " 'المشاهد',\n",
              " 'عن',\n",
              " 'وضع',\n",
              " 'الكثير',\n",
              " 'من',\n",
              " 'الملاحظات',\n",
              " 'او',\n",
              " 'الانتقادات',\n",
              " 'للتصميم',\n",
              " 'الاساسي',\n",
              " '.',\n",
              " 'وخلافا',\n",
              " 'للاعتقاد',\n",
              " 'السائد',\n",
              " 'فإن',\n",
              " 'لوريم',\n",
              " 'إيبسوم',\n",
              " 'ليس',\n",
              " 'نصا',\n",
              " 'عشوائيا،ً',\n",
              " 'بل',\n",
              " 'إن',\n",
              " 'له',\n",
              " 'جذور',\n",
              " 'في',\n",
              " 'الأدب',\n",
              " 'اللاتيني',\n",
              " 'الكلاسيكي',\n",
              " 'منذ',\n",
              " 'العام',\n",
              " '45',\n",
              " 'قبل',\n",
              " 'الميلاد.',\n",
              " 'من',\n",
              " 'كتاب',\n",
              " '\"',\n",
              " 'حول',\n",
              " 'أقاصي',\n",
              " 'الخير',\n",
              " 'والشر',\n",
              " '\"']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4rJ9nwwXydI",
        "outputId": "7a45bcc9-541c-4505-8753-8e1433650479"
      },
      "source": [
        "normalized_arabic_text = normalize_text(arabic_text)\n",
        "print(normalized_arabic_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "لوريم ايبسوم هُوَ نَمُوذَجَ اِفْتِراضِيٌّ يُوضَع فِي ال+ تَصامِيم لَ+ تَعَرَّضَ عَلَى ال+ عَمِيلِ لِ+ يَتَصَوَّر طَرِيقِ +هُ وَضْعِ ال+ نُصُوصِ بِ+ ال+ تَصامِيمِ سَواءُ كانَت تَصامِيم مَطْبُوعَة بروشور أَوْ فَ+ لِ+ إِير عَلَى سَبِيلِ ال+ مِثالِ أَوْ نَماذِجَ مَواقِعِ إِنْتِرْنِت وَ+ عِنْد مُوافَقَة ال+ عَمِيلِ ال+ مَبْدَئِيَّة عَلَى ال+ تَصْمِيمِ يَتِمّ أَزال +هُ هٰذا ال+ نَصِّ مِن ال+ تَصْمِيمِ وَ+ يَتِمّ وَضْعِ ال+ نُصُوصِ ال+ نِهائِيَّة ال+ مَطْلُوبَة لِ+ ال+ تَصْمِيم وَ يَقُول ال+ بَعْضُ أَنَّ وَضْعِ ال+ نُصُوصِ ال+ تَجْرِيبِيَّة بِ+ ال+ تَصْمِيم قَدْ تَشْغَل ال+ مَشاهِدِ عَن وَضْعِ ال+ كَثِيرَ مِن ال+ مُلاحَظاتِ أَوْ ال+ اِنْتِقاداتِ لِ+ ال+ تَصْمِيم ال+ أَساسِيِّ وَ+ خِلافاً لِ+ ال+ اِعْتِقاد ال+ سائِدِ فَ+ إِنَّ لوريم ايبسوم لَيِسَ نَصّاً عَشْوائِيّاً ، ً بَلْ أَنَّ لِ +هُ جُذُورُ فِي ال+ أَدَبِ ال+ لاتِينِيّ ال+ كلاسِيكِيِّ مُنْذُ ال+ عامِّ 45 قِبَلَ ال+ مِيلادِ مِن كِتابِ حَوْلَ أَقاصِي ال+ خَيْرِ وَ+ ال+ شَرِّ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiKAPUvxfnE6",
        "outputId": "1863009b-7bed-4fdc-a119-98c9d4825ad9"
      },
      "source": [
        "# Length of the output list\n",
        "len(normalized_arabic_text.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "148"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8dDQGoBtNIG"
      },
      "source": [
        "> ## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REJ5ENyEtTTZ"
      },
      "source": [
        "# Creating Our Tokenizer\n",
        "vocab_size = 50 #we can let it be 50 for now as we only have one example\n",
        "num_words=vocab_size\n",
        "\n",
        "\n",
        "tok = Tokenizer(num_words = vocab_size , oov_token='UNK')\n",
        "tok.fit_on_texts([normalized_arabic_text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UBnbP1QtjED",
        "outputId": "1ad86231-e7dd-41c2-becc-86405145c837"
      },
      "source": [
        "# Inspecting word index\n",
        "tok.word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'45': 76,\n",
              " 'UNK': 1,\n",
              " '،': 67,\n",
              " 'أَدَبِ': 71,\n",
              " 'أَزال': 44,\n",
              " 'أَساسِيِّ': 59,\n",
              " 'أَقاصِي': 81,\n",
              " 'أَنَّ': 21,\n",
              " 'أَوْ': 9,\n",
              " 'إِنَّ': 63,\n",
              " 'إِنْتِرْنِت': 40,\n",
              " 'إِير': 35,\n",
              " 'ال': 2,\n",
              " 'ايبسوم': 13,\n",
              " 'اِعْتِقاد': 61,\n",
              " 'اِفْتِراضِيٌّ': 24,\n",
              " 'اِنْتِقاداتِ': 58,\n",
              " 'بروشور': 34,\n",
              " 'بَعْضُ': 50,\n",
              " 'بَلْ': 69,\n",
              " 'بِ': 17,\n",
              " 'تَجْرِيبِيَّة': 51,\n",
              " 'تَشْغَل': 53,\n",
              " 'تَصامِيم': 15,\n",
              " 'تَصامِيمِ': 30,\n",
              " 'تَصْمِيم': 11,\n",
              " 'تَصْمِيمِ': 19,\n",
              " 'تَعَرَّضَ': 27,\n",
              " 'جُذُورُ': 70,\n",
              " 'حَوْلَ': 80,\n",
              " 'خَيْرِ': 82,\n",
              " 'خِلافاً': 60,\n",
              " 'سائِدِ': 62,\n",
              " 'سَبِيلِ': 36,\n",
              " 'سَواءُ': 31,\n",
              " 'شَرِّ': 83,\n",
              " 'طَرِيقِ': 29,\n",
              " 'عامِّ': 75,\n",
              " 'عَشْوائِيّاً': 66,\n",
              " 'عَلَى': 6,\n",
              " 'عَمِيلِ': 16,\n",
              " 'عَن': 55,\n",
              " 'عِنْد': 41,\n",
              " 'فَ': 18,\n",
              " 'فِي': 14,\n",
              " 'قَدْ': 52,\n",
              " 'قِبَلَ': 77,\n",
              " 'كانَت': 32,\n",
              " 'كلاسِيكِيِّ': 73,\n",
              " 'كَثِيرَ': 56,\n",
              " 'كِتابِ': 79,\n",
              " 'لاتِينِيّ': 72,\n",
              " 'لوريم': 12,\n",
              " 'لَ': 26,\n",
              " 'لَيِسَ': 64,\n",
              " 'لِ': 3,\n",
              " 'مَبْدَئِيَّة': 43,\n",
              " 'مَشاهِدِ': 54,\n",
              " 'مَطْبُوعَة': 33,\n",
              " 'مَطْلُوبَة': 48,\n",
              " 'مَواقِعِ': 39,\n",
              " 'مُلاحَظاتِ': 57,\n",
              " 'مُنْذُ': 74,\n",
              " 'مُوافَقَة': 42,\n",
              " 'مِثالِ': 37,\n",
              " 'مِن': 10,\n",
              " 'مِيلادِ': 78,\n",
              " 'نَصّاً': 65,\n",
              " 'نَصِّ': 46,\n",
              " 'نَماذِجَ': 38,\n",
              " 'نَمُوذَجَ': 23,\n",
              " 'نُصُوصِ': 8,\n",
              " 'نِهائِيَّة': 47,\n",
              " 'هُ': 7,\n",
              " 'هُوَ': 22,\n",
              " 'هٰذا': 45,\n",
              " 'وَ': 4,\n",
              " 'وَضْعِ': 5,\n",
              " 'يَتَصَوَّر': 28,\n",
              " 'يَتِمّ': 20,\n",
              " 'يَقُول': 49,\n",
              " 'يُوضَع': 25,\n",
              " 'ً': 68}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9mGnOppwMI7",
        "outputId": "c2ff2f89-211d-4dc4-817f-c1b3886b5a2c"
      },
      "source": [
        "# This is the document with its words index in the document\n",
        "tok.texts_to_sequences([normalized_arabic_text.split()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[12,\n",
              "  13,\n",
              "  22,\n",
              "  23,\n",
              "  24,\n",
              "  25,\n",
              "  14,\n",
              "  1,\n",
              "  15,\n",
              "  1,\n",
              "  27,\n",
              "  6,\n",
              "  1,\n",
              "  16,\n",
              "  1,\n",
              "  28,\n",
              "  29,\n",
              "  1,\n",
              "  5,\n",
              "  1,\n",
              "  8,\n",
              "  1,\n",
              "  1,\n",
              "  30,\n",
              "  31,\n",
              "  32,\n",
              "  15,\n",
              "  33,\n",
              "  34,\n",
              "  9,\n",
              "  1,\n",
              "  1,\n",
              "  35,\n",
              "  6,\n",
              "  36,\n",
              "  1,\n",
              "  37,\n",
              "  9,\n",
              "  38,\n",
              "  39,\n",
              "  40,\n",
              "  1,\n",
              "  41,\n",
              "  42,\n",
              "  1,\n",
              "  16,\n",
              "  1,\n",
              "  43,\n",
              "  6,\n",
              "  1,\n",
              "  19,\n",
              "  20,\n",
              "  44,\n",
              "  1,\n",
              "  45,\n",
              "  1,\n",
              "  46,\n",
              "  10,\n",
              "  1,\n",
              "  19,\n",
              "  1,\n",
              "  20,\n",
              "  5,\n",
              "  1,\n",
              "  8,\n",
              "  1,\n",
              "  47,\n",
              "  1,\n",
              "  48,\n",
              "  1,\n",
              "  1,\n",
              "  11,\n",
              "  4,\n",
              "  49,\n",
              "  1,\n",
              "  1,\n",
              "  21,\n",
              "  5,\n",
              "  1,\n",
              "  8,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  11,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  5,\n",
              "  1,\n",
              "  1,\n",
              "  10,\n",
              "  1,\n",
              "  1,\n",
              "  9,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  11,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  21,\n",
              "  3,\n",
              "  1,\n",
              "  1,\n",
              "  14,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  10,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz0ZobkXw5Cm",
        "outputId": "ff9f40dc-14e1-4c64-d1c1-eae6d1b65428"
      },
      "source": [
        "# This the representation in binary\n",
        "# it's all 1 because we only have one example\n",
        "tok.texts_to_matrix([normalized_arabic_text], mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBNQYGw_xuWq"
      },
      "source": [
        "__The text is now ready to be fed in a neural network__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIDGWLuHx4uO"
      },
      "source": [
        "# Citation and references"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mfw8ynTlsJ7"
      },
      "source": [
        "# https://colab.research.google.com/drive/1Y3qCbD6Gw1KEw-lixQx1rI6WlyWnrnDS?usp=sharing#scrollTo=Upf_eeVE0889\n",
        "# https://wti.kaust.edu.sa/upcoming-events/Machine-Learning-Arabic-NLP-Webinar/ml-webinar-2020-keynote-1\n",
        "# University of New York Abudhabi\n",
        "\n",
        "# @inproceedings{obeid-etal-2020-camel,\n",
        "#    title = \"{CAM}e{L} Tools: An Open Source Python Toolkit for {A}rabic Natural Language Processing\",\n",
        "#    author = \"Obeid, Ossama  and\n",
        "#       Zalmout, Nasser  and\n",
        "#       Khalifa, Salam  and\n",
        "#       Taji, Dima  and\n",
        "#       Oudah, Mai  and\n",
        "#       Alhafni, Bashar  and\n",
        "#       Inoue, Go  and\n",
        "#       Eryani, Fadhl  and\n",
        "#       Erdmann, Alexander  and\n",
        "#       Habash, Nizar\",\n",
        "#    booktitle = \"Proceedings of the 12th Language Resources and Evaluation Conference\",\n",
        "#    month = may,\n",
        "#    year = \"2020\",\n",
        "#    address = \"Marseille, France\",\n",
        "#    publisher = \"European Language Resources Association\",\n",
        "#    url = \"https://www.aclweb.org/anthology/2020.lrec-1.868\",\n",
        "#    pages = \"7022--7032\",\n",
        "#    abstract = \"We present CAMeL Tools, a collection of open-source tools for Arabic natural language processing in Python. CAMeL Tools currently provides utilities for pre-processing, morphological modeling, Dialect Identification, Named Entity Recognition and Sentiment Analysis. In this paper, we describe the design of CAMeL Tools and the functionalities it provides.\",\n",
        "#    language = \"English\",\n",
        "#    ISBN = \"979-10-95546-34-4\",\n",
        "# }"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}