{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT for style completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling,\\\n",
    "    GPT2LMHeadModel, pipeline, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# Set the EOS token as the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds_data = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./data/PDS2.txt',\n",
    "    block_size=32\n",
    ").train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  200, 47231,  6418,   286,  6060,  5800,   198, 12211,  5061,   198,\n",
       "           198,    32, 31516,   338,  5698,   284, 13905,  7605,   290,  4583,\n",
       "           284,   198, 11249,   304,   171,   105,   222, 13967,  1366,    12,\n",
       "         15808,  5479]),\n",
       " torch.Size([32]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pds_data[0], pds_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Č', 'Prin', 'ciples', 'Ġof', 'ĠData', 'ĠScience', 'Ċ', 'Second', 'ĠEdition', 'Ċ', 'Ċ', 'A', 'Ġbeginner', \"'s\", 'Ġguide', 'Ġto', 'Ġstatistical', 'Ġtechniques', 'Ġand', 'Ġtheory', 'Ġto', 'Ċ', 'build', 'Ġe', 'ï', '¬', 'Ģ', 'ective', 'Ġdata', '-', 'driven', 'Ġapplications']\n",
      "\fPrinciples of Data Science\n",
      "Second Edition\n",
      "\n",
      "A beginner's guide to statistical techniques and theory to\n",
      "build eﬀective data-driven applications\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(pds_data[0]))\n",
    "print(tokenizer.decode(pds_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[   40,   716,   281,  5128],\n",
       "         [ 2396,   716,   314, 50256]]]), 'attention_mask': tensor([[[1, 1, 1, 1],\n",
       "         [1, 1, 1, 0]]]), 'labels': tensor([[[  40,  716,  281, 5128],\n",
       "         [2396,  716,  314, -100]]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "# Example\n",
    "data_collator(\n",
    "    [tokenizer(['I am an input','So am I'], padding=True, truncation=True, return_tensors=\"pt\")]\n",
    ")\n",
    "### Reminder: Labels are shifted *inside* the GPT model so we don't need to worry about that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "pretrained_generator = pipeline(\n",
    "    'text-generation', model=model, tokenizer='gpt2',\n",
    "    config={\n",
    "        'max_length':200, 'do_sample':True, 'top_p':0.9, 'temperature':0.7, 'top_k':10\n",
    "    } # precedence to top_k over top_p\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "A dataset shows the relationships between high-frequency, midlevel, and low-frequency energy consumption patterns and energy expenditures by age, sex, smoking and body mass index, from 1975 to 2009. Table 4. Energy Expenditures Per Woman Age Weighted\n",
      "\n",
      "--------------------\n",
      "A dataset shows the relationships among each of the four major economic outcomes, from the US Economy to the US Environment (Fig 1); also the income and wealth inequality of workers and corporations; and the degree to which a business-as-usual approach to\n",
      "\n",
      "--------------------\n",
      "A dataset shows the relationships between the effects of both smoking and heart disease on body weight and diabetes. The most commonly cited study for the relationship was the Health Professionals Follow-up Study, which was initiated at the Massachusetts Institute of Technology in Boston by\n"
     ]
    }
   ],
   "source": [
    "for generated_sequence in pretrained_generator('A dataset shows the relationships', num_return_sequences=3):\n",
    "    print('\\n--------------------')\n",
    "    print(generated_sequence['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce GTX 1660 Ti\n",
      "Total memory: 6.44 GB\n",
      "Allocated memory: 0.00 GB\n",
      "Cached memory: 0.00 GB\n",
      "Free memory: 6.44 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "def show_cuda_space_info():\n",
    "    if torch.cuda.is_available():\n",
    "        # Get the CUDA device name\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using device:\", torch.cuda.get_device_name(device))\n",
    "\n",
    "        # Memory allocation and caching are dynamic in PyTorch, but you can get approximate memory usage as follows\n",
    "        total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "        allocated_memory = torch.cuda.memory_allocated(device)\n",
    "        cached_memory = torch.cuda.memory_reserved(device)\n",
    "        free_memory = total_memory - (allocated_memory + cached_memory)\n",
    "\n",
    "        print(f\"Total memory: {total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"Allocated memory: {allocated_memory / 1e9:.2f} GB\")\n",
    "        print(f\"Cached memory: {cached_memory / 1e9:.2f} GB\")\n",
    "        print(f\"Free memory: {free_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "show_cuda_space_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:03<00:00,  8.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.955997467041016,\n",
       " 'eval_runtime': 31.527,\n",
       " 'eval_samples_per_second': 29.816,\n",
       " 'eval_steps_per_second': 0.952}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt2_pds',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=len(pds_data.examples) // 5,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=pds_data.examples[:int(len(pds_data.examples)*.8)],\n",
    "    eval_dataset=pds_data.examples[int(len(pds_data.examples)*.8):]\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce GTX 1660 Ti\n",
      "Total memory: 6.44 GB\n",
      "Allocated memory: 0.52 GB\n",
      "Cached memory: 1.38 GB\n",
      "Free memory: 4.54 GB\n"
     ]
    }
   ],
   "source": [
    "show_cuda_space_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 50/354 [00:21<01:59,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7274, 'learning_rate': 2.6624068157614487e-06, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 100/354 [00:42<01:39,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.302, 'learning_rate': 5.324813631522897e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 33%|███▎      | 118/354 [00:53<01:22,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.095743656158447, 'eval_runtime': 3.7433, 'eval_samples_per_second': 251.119, 'eval_steps_per_second': 8.014, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 150/354 [01:18<01:20,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9368, 'learning_rate': 7.987220447284345e-06, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 200/354 [01:39<01:01,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.795, 'learning_rate': 1.0649627263045795e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 236/354 [01:57<00:39,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.8664770126342773, 'eval_runtime': 3.7349, 'eval_samples_per_second': 251.68, 'eval_steps_per_second': 8.032, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 250/354 [02:14<00:46,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6292, 'learning_rate': 1.3312034078807243e-05, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 300/354 [02:34<00:20,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5164, 'learning_rate': 1.597444089456869e-05, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 350/354 [02:53<00:01,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.407, 'learning_rate': 1.8636847710330137e-05, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 354/354 [02:58<00:00,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7757225036621094, 'eval_runtime': 3.6745, 'eval_samples_per_second': 255.816, 'eval_steps_per_second': 8.164, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "100%|██████████| 354/354 [03:12<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 192.0601, 'train_samples_per_second': 58.669, 'train_steps_per_second': 1.843, 'train_loss': 3.8968389802059886, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=354, training_loss=3.8968389802059886, metrics={'train_runtime': 192.0601, 'train_samples_per_second': 58.669, 'train_steps_per_second': 1.843, 'train_loss': 3.8968389802059886, 'epoch': 3.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_generator = pipeline(\n",
    "    'text-generation', model=GPT2LMHeadModel.from_pretrained('./gpt2_pds/'), tokenizer='gpt2',\n",
    "    config={\n",
    "        'max_length':200, 'do_sample':True, 'top_p':0.9, 'temperature':0.7, 'top_k':10\n",
    "    } # precedence to top_k over top_p\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "A dataset shows the relationships between words and locations – this data is usually used as a cross-validation tool.\n",
      "Word Counts\n",
      "The more data you have, the closer it is to being a corpus. The more categorical data there is\n",
      "\n",
      "--------------------\n",
      "A dataset shows the relationships between variables within the\n",
      "data set. It is very useful for understanding\n",
      "that\n",
      "we cannot use categorical variables only within a data set and only within a\n",
      "quantitative set (such as\n",
      "a plot). This means\n",
      "\n",
      "--------------------\n",
      "A dataset shows the relationships between these variables. To perform this, we can create the following transformations:\n",
      "Let's say that we have a column of\n",
      "number n that contains\n",
      "two random variables, 1 and n2.\n",
      "Now we create columns\n"
     ]
    }
   ],
   "source": [
    "for generated_sequence in finetuned_generator('A dataset shows the relationships', num_return_sequences=3):\n",
    "    print('\\n--------------------')\n",
    "    print(generated_sequence['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT for code dictation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer,\\\n",
    "        GPT2LMHeadModel, pipeline\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>LaTeX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>integral from a to b of x squared</td>\n",
       "      <td>\\int_{a}^{b} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>integral from negative 1 to 1 of x squared</td>\n",
       "      <td>\\int_{-1}^{1} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>integral from negative 1 to infinity of x cubed</td>\n",
       "      <td>\\int_{-1}^{\\inf} x^3 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>integral from 0 to infinity of x squared</td>\n",
       "      <td>\\int_{0}^{\\inf} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>integral from 0 to infinity of y squared</td>\n",
       "      <td>\\int_{0}^{\\inf} y^2 \\,dy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           English                      LaTeX\n",
       "0                integral from a to b of x squared      \\int_{a}^{b} x^2 \\,dx\n",
       "1       integral from negative 1 to 1 of x squared     \\int_{-1}^{1} x^2 \\,dx\n",
       "2  integral from negative 1 to infinity of x cubed  \\int_{-1}^{\\inf} x^3 \\,dx\n",
       "3         integral from 0 to infinity of x squared   \\int_{0}^{\\inf} x^2 \\,dx\n",
       "4         integral from 0 to infinity of y squared   \\int_{0}^{\\inf} y^2 \\,dy"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/english_to_latex.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# Set the EOS token as the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "CONVERSION_PROMPT = 'LCT\\n'\n",
    "CONVERSION_TOKEN = 'LaTeX:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: integral from a to b of x squared\n",
      "LaTeX: \\int_{a}^{b} x^2 \\,dx\n"
     ]
    }
   ],
   "source": [
    "training_examples = f'{CONVERSION_PROMPT}English: ' + data['English'] + '\\n' + CONVERSION_TOKEN + ' ' + data['LaTeX']\n",
    "print(training_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LCT\\nEnglish: integral from a to b of x square...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LCT\\nEnglish: integral from negative 1 to 1 of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  LCT\\nEnglish: integral from a to b of x square...\n",
       "1  LCT\\nEnglish: integral from negative 1 to 1 of..."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_df = pd.DataFrame({'text':training_examples})\n",
    "task_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:01<00:00, 28.22 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_data = Dataset.from_pandas(task_df)\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples['text'],truncation=True)\n",
    "\n",
    "latex_data = latex_data.map(preprocess, batched=True)\n",
    "latex_data = latex_data.train_test_split(train_size=.8)\n",
    "latex_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 31.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.3224687576293945,\n",
       " 'eval_runtime': 0.3769,\n",
       " 'eval_samples_per_second': 26.533,\n",
       " 'eval_steps_per_second': 2.653}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./english_to_latex',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=10,\n",
    "    # warmup_steps=len(pds_data.examples) // 5,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=latex_data['train'],\n",
    "    eval_dataset=latex_data['test']\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 10%|█         | 20/200 [00:03<00:18,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1638877391815186, 'eval_runtime': 0.1064, 'eval_samples_per_second': 94.014, 'eval_steps_per_second': 9.401, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|██        | 40/200 [00:15<00:22,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9504401087760925, 'eval_runtime': 0.1042, 'eval_samples_per_second': 95.95, 'eval_steps_per_second': 9.595, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 52/200 [00:37<01:11,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5005, 'learning_rate': 3.7500000000000003e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 30%|███       | 60/200 [00:38<00:25,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8839876055717468, 'eval_runtime': 0.1061, 'eval_samples_per_second': 94.232, 'eval_steps_per_second': 9.423, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 40%|████      | 80/200 [01:00<00:21,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8503780364990234, 'eval_runtime': 0.1062, 'eval_samples_per_second': 94.144, 'eval_steps_per_second': 9.414, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 100/200 [01:25<00:19,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.565, 'learning_rate': 2.5e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 50%|█████     | 100/200 [01:25<00:19,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.861503005027771, 'eval_runtime': 0.0627, 'eval_samples_per_second': 159.478, 'eval_steps_per_second': 15.948, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 120/200 [01:45<00:12,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8159395456314087, 'eval_runtime': 0.1088, 'eval_samples_per_second': 91.901, 'eval_steps_per_second': 9.19, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 70%|███████   | 140/200 [02:08<00:10,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8706914186477661, 'eval_runtime': 0.0963, 'eval_samples_per_second': 103.826, 'eval_steps_per_second': 10.383, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 152/200 [02:30<00:23,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4623, 'learning_rate': 1.25e-05, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|████████  | 160/200 [02:30<00:07,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8743969798088074, 'eval_runtime': 0.1056, 'eval_samples_per_second': 94.719, 'eval_steps_per_second': 9.472, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 90%|█████████ | 180/200 [02:53<00:03,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8968345522880554, 'eval_runtime': 0.095, 'eval_samples_per_second': 105.265, 'eval_steps_per_second': 10.527, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:16<00:00,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.357, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 200/200 [03:16<00:00,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9047366976737976, 'eval_runtime': 0.0621, 'eval_samples_per_second': 160.992, 'eval_steps_per_second': 16.099, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:35<00:00,  5.40it/s]There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "100%|██████████| 200/200 [03:50<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 230.6813, 'train_samples_per_second': 1.734, 'train_steps_per_second': 0.867, 'train_loss': 0.7211941337585449, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.7211941337585449, metrics={'train_runtime': 230.6813, 'train_samples_per_second': 1.734, 'train_steps_per_second': 0.867, 'train_loss': 0.7211941337585449, 'epoch': 10.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 32.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8159395456314087,\n",
       " 'eval_runtime': 0.105,\n",
       " 'eval_samples_per_second': 95.251,\n",
       " 'eval_steps_per_second': 9.525,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\__ing\\llming_1\\.venv\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "calculus_data = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./data/Calculus_Made_Easy_by_Silvanus_P._Thompson.txt',\n",
    "    block_size=32\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
